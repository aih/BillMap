:toc:

# FlatGov: A https://demandprogress.org[Demand Progress] Project 
Utilities and applications for the FlatGov project by Demand Progress


This repository contains:

* A web application showing information for a given bill (Django/Python)
* Utilities to scrape and process bill data (Python)

Both components are described below.

## Web Application

The FlatGov web application is built using the Django/Python web application framework. The application is contained in the `server_py` directory of this repository. It makes use of data that is processed using the scrapers and scripts described in the <<Data>> and <<Metadata>> sections below.

### Quickstart

#### Install Python dependencies

Create a new Python virtual environment. You can use `venv`, `virtualenv` or preferably https://github.com/pyenv/pyenv-virtualenv[`pyenv virtualenv`], which requires installing https://github.com/pyenv/pyenv[pyenv] first).

Create the environment (with pyenv virtualenv):
```bash
$ pyenv install 3.7
$ pyenv virtualenv 3.7 flatgov
```

Activate the environment
```bash
$ pyenv activate flatgov
```

Then load the `requirements.txt` into the virtual environment:

```bash
$ cd server_py
$ pip install -r requirements.txt
```

```bash
$ cd scripts
$ pip install -r requirements.txt
```

The requirements for `scripts` and `server_py` are separate. But for simplicity, it's probably best to load them both in a single virtual environment.

#### Create .env file 

Copy `server_py/flatgov/.env-sample` to `server_py/flatgov/.env`, and change the `SECRET_KEY` defined in that file.

#### Add data to `data_json`
Copy the `json_data` directory, containing sample data, into the `server_py/flatgov` directory. Your directory will then look like:

```bash
server_py
       |
       -flatgov
          |
         json_data
          |
          -billsMeta.json
          -titlesIndex.json
          -relatedBills.json
          ...
```

Alternately, you can generate the `billsMeta.json` and `titlesIndex.json` files using the scrapers and scripts described in 
<<Data>> and <<Metadata>>.

#### Run the Django application

Run the application from `server_py/flatgov` (within the Python virtual environment you created above):

```bash
$ cd server_py/flatgov
$ python manage.py runserver
```

This will serve the application on localhost:8000. Currently, there is no home page; only pages for individual bills, at, e.g.:
http://localhost:8000/bills/116hr1500enr

## Data

### Where to store data

This repository contains scripts in the `/scripts` directory that process bill data. The default location for the data is in a directory that is a parent of this repository. To set that up, create a directory, `FlatgovDir`. *Within* that directory, clone this repository:

```bash
$ mkdir FlatgovDir
$ cd FlatgovDir
$ git clone https://github.com/aih/FlatGov.git
(requires Github credentials)
```

### Bulk downloads: bill metadata

The core metadata that will be used for this project can be downloaded in bulk from ProPublica^TM^ here: https://www.propublica.org/datastore/dataset/congressional-data-bulk-legislation-bills

Bulk historical metadata is available for one-year ranges. Data for the current Congress is updated twice daily. The metadata is sufficient to test many of the functions in this library. Bill text comparison requires bulk download or scraping of the text.

Each download creates a `.zip` file for the congress downloaded (e.g. `116.zip`). When unzipped, this creates a directory of the form:

`congress -> data -> 116`

To start, it is sufficient to download a few recent congresses (e.g. `116, 115, 114, 113`). Unzip, and copy the deepest directory into a single hierarchy so that you have:

```bash
FlatGovDir
   |
   -congress
       |
       -data
          |
          -116
          -115
          -114
          -113
```


### Scraping bill text and metadata
#### Install `unitedstates/congress` repository

The text of bills can be scraped with the Python project here: `https://github.com/unitedstates/congress`. First, clone this repository as a child of the `FlatGovDir` directory above, and a sibling of `congress`. In this way, running the scraper will fill out the text data within the `congress` directory.

```
$ cd /path/to/FlatGovDir
$ git clone https://github.com/unitedstates/congress.git
(no credentials needed-- it is an open repo)
```

#### Install scraper dependencies

Install the `congress` Python virtual environment, install the requirements (`pip install requirements.txt`). The scrapers were built with Python 2.7 and have not been upgraded; updates may be needed for a production environment, but the `@unitedstates/congress` scraper is sufficient to gather the baseline data to test the utilities in this repository.

NOTE: Scraping the initial data can be *very* time-consuming (most of a day, depending on your internet download speeds). To get started, it is worth finding a source for bulk downloads of the text, if possible.

On MacOS (Catalina), installing the `congress` requirements involved a few adjustments:

1. Install OpenSSL 1.02 with Homebrew. The latest OpenSSL (>1.1) causes problems with certain requirements; unfortunately, version 1.0.0 also failed. A script was set up by a Github user to install version 1.0.2.

`brew uninstall openssl --ignore-dependencies; brew uninstall openssl --ignore-dependencies; brew uninstall libressl --ignore-dependencies; brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/8b9d6d688f483a0f33fcfc93d433de501b9c3513/Formula/openssl.rb;`

2. Link the OpenSSL libraries

```
export LDFLAGS=-L/usr/local/opt/openssl/lib
export CPPFLAGS=-I/usr/local/opt/openssl/include
```

3. Install `pytz`, `pep517` and `cryptography` directly

```bash
pip install pytz
pip install pep517
pip install cryptography
```

4. Install requirements

From the `congress` repository directory, `pip install -r requirements.txt`

### Run the scraper

```bash
./run govinfo --bulkdata=BILLSTATUS
./run bills
```

When running initially, I got an error because the bulk directories had not been made. To unzip the files manually in all directories:

`find . -name "*.zip" | xargs -P 5 -I fileName sh -c 'unzip -o -d "$(dirname "fileName")/$(basename -s .zip "fileName")" "fileName"'`

## Metadata

### Quickstart

Once the files are stored in the `congress/data` directory, you can use the Python scripts in the `scripts` directory to generate various metadata files can be generated that combine and index data from the individual `data.json` files found in `congress/data`. A separate `scripts/README.adoc` file will describe the usage of those scripts in detail. In general, the scripts can be run from the command line as follows:

```bash
$ cd scripts
$ ls | grep "[^_].py$"
billdata.py
process_bill_meta.py
relatedBills.py

$ python billdata.py #runs the billdata script
...
$ python process_bill_meta.py # combines data from the results of the above script, to get titles
...
$ python relatedBills.py # processes the data above to generate a related bills json

```

### Background

a metadata file can be created with the following information: *Congress*, *Name*, *Path to XML or Text*, *Sponsors*. As an initial form, the metadata file will be:

billdata.json
```javascript
[ 
  116hr1ih: {
          name: '116hr1ih',
          sponsors: ['name1', 'name2'...],
          titles: [],
          titles_whole_bill: [],
          path_xml: '...',
          path_text: '...',
           }
           ...
  115hjres31: {
     }
]
```

Where 'titles' includes all titles and 'full_titles' includes those where `"is_for_portion": false` (see below). 

### Cosponsors
This information is available for each bill in the `data.json` file. Two key fields in `sponsors` are `name` and `bioguide_id`

### Bill Titles
This information is available for each bill (and version) in the `data.json` file. For example, in `/congress/data/116/bills/hr/hr3/data.json`. After collecting titles for each bill, a reverse index can be created, with the title as key and an array of billnumbers as value. This will identify the bills across congresses that share identical titles.

The title information in `data.json` is of the form:

```javascript
"titles": [
    {
      "as": "introduced", 
      "is_for_portion": false, 
      "title": "INVEST in America Act", 
      "type": "short"
    }, 
    {
      "as": "introduced", 
      "is_for_portion": false, 
      "title": "INVEST in America Act", 
      "type": "short"
    }, 
    {
      "as": "introduced", 
      "is_for_portion": false, 
      "title": "Investing in a New Vision for the Environment and Surface Transportation in America Act", 
      "type": "short"
    } ...
]
```

## Finding Related Bills

### Similarity measures: simple
* Simple similarity measures are obtained using the relatedBills.py file which expands upon the functionality generated by the files: billdata.py and process_bill_meta.py. 
* relatedBills.py uses the getRelatedBills() as a higher order function containing several functions that obtain simple similarity measures.

A few 'simple' measures can be taken of similarity. Bills which share:

* Identical titles
* Very similar titles (e.g. all but the year)
* Identical sponsor lists
* Significant overlap in sponsors

This can be represented in a summary JSON of the form:
`relatedBills.json`

```javascript
  116s130: {
    same_titles: ['116hr201', ...]
  }
]
```

OR

```javascript
116s130: [
  { billCongressTypeNumber: '116hr201' 
    cosponsors: [bioguide_id1, bioguide_id2],
    titles: ['Shared Title 1', 'Shared Title 2', etc.]
    similar_title: ['Similar (nonidentical) Title 1', 'Similar (nonidentical) Title 2', etc.]
  }...
  ],

]
          
```

#### (Same)Titles
It does this by creating a billnumber index with the bill metadata, and any similarity measures will subsequently be attributed to its corresponding number in the index. For example, after the index is created,a “getSameTitles” function is run, which loops through the index and creates a list of titles for that billNumber. A bill number with more than one title would then indicate that the bill has more than one version of itself. Identical titles would indicate identical bills, with different bill numbers.

#### Cosponsors
(to do)

#### Similar Title
(to )do

### Similarity calculation

For any bill (e.g. 116hr100ih), we want to find related bills for previous congresses. Related bills are listed for the same congress in Congress.gov, e.g. https://www.congress.gov/bill/116th-congress/house-bill/2/related-bills?q={"search":["hr2"]}&r=1&s=3.  There are many ways of calculating similarity. We will use an overall similarity measure, with a value between 0 and 1. The similarity is calculated as a linear combination of matching functions of the form:

`Similarity(bill~1~,bill~2~) = normalize(w~1~*f~1~(bill~1~, bill~2~) + w~2~*f~2~(bill1, bill2) + ...)`

NOTE: a bill text similarity engine is here https://github.com/govtrack/govtrack.us-web/blob/master/analysis/text_incorporation.py

Each similarity function has the following properties:

name:: a unique name for the feature being measured
function:: a function that takes in metadata or text from the two bills and returns a value between 0 and 1
minthreshold:: the minimum value of the function that will be counted; if the function value is less than minthreshold, it is set to 0
maxthreshold:: 
a threshold that sets the whole function to 'true'. If the function is greater than this value, the two bills are considered to be a match. For example, if the titles are identical, the two bills will be considered a match, regardless of the value of the other functions.

